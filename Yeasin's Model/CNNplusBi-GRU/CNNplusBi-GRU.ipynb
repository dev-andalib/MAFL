{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95376c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu118\n",
      "CUDA Available: True\n",
      "Device Count: 1\n",
      "Current Device Index: 0\n",
      "Device Name: NVIDIA GeForce GTX 1650\n",
      "Active Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"Device Count:\", torch.cuda.device_count())\n",
    "print(\"Current Device Index:\", torch.cuda.current_device())\n",
    "print(\"Device Name:\", torch.cuda.get_device_name(torch.cuda.current_device()) if torch.cuda.is_available() else \"CPU Only\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Active Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8f55a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\T2430479\\miniconda3\\envs\\MAFL\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b3dea65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data_utils\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_auc_score\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "658749e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Dataset Initialization\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mD:\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mT24\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mYeasin\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ms Model\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mDataset\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mCIC-ToN-IoT-V2.parquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset loaded successfully with shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m df.head(\u001b[32m10\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\T2430479\\miniconda3\\envs\\MAFL\\Lib\\site-packages\\pandas\\io\\parquet.py:653\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    500\u001b[39m \u001b[38;5;129m@doc\u001b[39m(storage_options=_shared_docs[\u001b[33m\"\u001b[39m\u001b[33mstorage_options\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_parquet\u001b[39m(\n\u001b[32m    502\u001b[39m     path: FilePath | ReadBuffer[\u001b[38;5;28mbytes\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    510\u001b[39m     **kwargs,\n\u001b[32m    511\u001b[39m ) -> DataFrame:\n\u001b[32m    512\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    513\u001b[39m \u001b[33;03m    Load a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[32m    514\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    650\u001b[39m \u001b[33;03m    1    4    9\u001b[39;00m\n\u001b[32m    651\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m     impl = \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m use_nullable_dtypes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib.no_default:\n\u001b[32m    656\u001b[39m         msg = (\n\u001b[32m    657\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe argument \u001b[39m\u001b[33m'\u001b[39m\u001b[33muse_nullable_dtypes\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is deprecated and will be removed \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    658\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33min a future version.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    659\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\T2430479\\miniconda3\\envs\\MAFL\\Lib\\site-packages\\pandas\\io\\parquet.py:68\u001b[39m, in \u001b[36mget_engine\u001b[39m\u001b[34m(engine)\u001b[39m\n\u001b[32m     65\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m     66\u001b[39m             error_msgs += \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m - \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(err)\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     69\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to find a usable engine; \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     70\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtried using: \u001b[39m\u001b[33m'\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mfastparquet\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     71\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mA suitable version of \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     72\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpyarrow or fastparquet is required for parquet \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     73\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msupport.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTrying to import the above resulted in these errors:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     75\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msgs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     76\u001b[39m     )\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m PyArrowImpl()\n",
      "\u001b[31mImportError\u001b[39m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
     ]
    }
   ],
   "source": [
    "# Dataset Initialization\n",
    "df = pd.read_parquet(\"D:\\T24\\Yeasin's Model\\Dataset\\CIC-ToN-IoT-V2.parquet\")\n",
    "print(f\"Dataset loaded successfully with shape: {df.shape}\")\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71999256",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploratory Data Analysis (EDA)\n",
    "\n",
    "missing = df.isnull().sum()\n",
    "print(\"Missing values per column:\")\n",
    "print(missing[missing > 0])\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='Label', data=df)\n",
    "plt.title(\"Binary Class Distribution (Benign vs Attack)\")\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "df['Attack'].value_counts().plot(kind='bar')\n",
    "plt.title(\"Multiclass Attack Type Distribution\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation Matrix with Label.\n",
    "numeric_cols = df.select_dtypes(include=['int8', 'int16', 'int32', 'int64', 'float32', 'float64']).columns.tolist()\n",
    "numeric_cols.remove('Label')  # remove target label from predictors\n",
    "corr_with_label = df[numeric_cols].corrwith(df['Label']).abs().sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "corr_with_label.plot(kind='bar')\n",
    "plt.title(\"Feature Correlation with Label\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdc3e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PreProcessing\n",
    "\n",
    "le_attack = LabelEncoder()\n",
    "df['Attack_encoded'] = le_attack.fit_transform(df['Attack'])\n",
    "\n",
    "\n",
    "features = df.drop(columns=['Label', 'Attack', 'Attack_encoded']) #dropped both labels temporarily\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Final feature matrix (as DataFrame)\n",
    "X = pd.DataFrame(X_scaled, columns=features.columns)\n",
    "\n",
    "# Binary and multi-class targets\n",
    "y_bin = df['Label']\n",
    "y_multi = df['Attack_encoded']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Binary target shape: {y_bin.shape}\")\n",
    "print(f\"Multiclass target shape: {y_multi.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf6023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bin, X_test_bin, y_train_bin, y_test_bin = train_test_split(\n",
    "    X, y_bin, test_size=0.2, random_state=42, stratify=y_bin\n",
    ")\n",
    "print(\"Class distribution before SMOTE:\", Counter(y_train_bin))\n",
    "\n",
    "#SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "# X_train_bin_smote, y_train_bin_smote = smote.fit_resample(X_train_bin, y_train_bin)\n",
    "X_train_bin_smote, y_train_bin_smote = X_train_bin, y_train_bin\n",
    "\n",
    "print(\"Class distribution after SMOTE:\", Counter(y_train_bin_smote))\n",
    "print(f\"Train shape: {X_train_bin_smote.shape}, Test shape: {X_test_bin.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54106f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_bin_smote.values, dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test_bin.values, dtype=torch.float32).to(device)\n",
    "\n",
    "batch_size = 512\n",
    "train_loader = data_utils.DataLoader(\n",
    "    data_utils.TensorDataset(X_train_tensor, X_train_tensor),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "input_dim = X_train_bin_smote.shape[1]\n",
    "autoencoder = Autoencoder(input_dim).to(device)\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "epochs = 50\n",
    "autoencoder.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for xb, _ in train_loader:\n",
    "        output = autoencoder(xb)\n",
    "        loss = criterion(output, xb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_loader):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1564b459",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed_test = autoencoder(X_test_tensor).cpu().numpy()\n",
    "\n",
    "reconstruction_error = np.mean((X_test_bin.values - reconstructed_test) ** 2, axis=1)\n",
    "\n",
    "print(f\"Mean Reconstruction Error: {reconstruction_error.mean():.6f}\")\n",
    "print(f\"Std of Reconstruction Error: {reconstruction_error.std():.6f}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(reconstruction_error, bins=50, kde=True)\n",
    "plt.title(\"Distribution of Reconstruction Error (MSE per Sample)\")\n",
    "plt.xlabel(\"MSE\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8f8fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    X_train_encoded = autoencoder.encoder(X_train_tensor).cpu().numpy()\n",
    "    X_test_encoded = autoencoder.encoder(X_test_tensor).cpu().numpy()\n",
    "\n",
    "print(\"Encoded feature shapes:\")\n",
    "print(\"Train:\", X_train_encoded.shape)\n",
    "print(\"Test :\", X_test_encoded.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fafe138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Preparation.\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Reshape encoded features: [N, 32] → [N, 32, 1]\n",
    "X_train_seq = torch.tensor(X_train_encoded[:, :, np.newaxis], dtype=torch.float32)\n",
    "X_test_seq = torch.tensor(X_test_encoded[:, :, np.newaxis], dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_bin_smote.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_bin.values, dtype=torch.float32)\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(X_train_seq, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_seq, y_test_tensor)\n",
    "\n",
    "batch_size = 512\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "\n",
    "X_train_mc, X_test_mc, y_train_mc, y_test_mc = train_test_split(\n",
    "    X, y_multi, test_size=0.2, random_state=42, stratify=y_multi\n",
    ")\n",
    "\n",
    "\n",
    "X_train_mc_tensor = torch.tensor(\n",
    "    autoencoder.encoder(torch.tensor(X_train_mc.values, dtype=torch.float32).to(device)).detach().cpu().numpy()[:, :, np.newaxis], \n",
    "    dtype=torch.float32\n",
    ")\n",
    "X_test_mc_tensor = torch.tensor(\n",
    "    autoencoder.encoder(torch.tensor(X_test_mc.values, dtype=torch.float32).to(device)).detach().cpu().numpy()[:, :, np.newaxis], \n",
    "    dtype=torch.float32\n",
    ")\n",
    "y_train_mc_tensor = torch.tensor(y_train_mc.values, dtype=torch.long)\n",
    "y_test_mc_tensor = torch.tensor(y_test_mc.values, dtype=torch.long)\n",
    "\n",
    "# Multi-class loaders\n",
    "train_dataset_mc = TensorDataset(X_train_mc_tensor, y_train_mc_tensor)\n",
    "test_dataset_mc = TensorDataset(X_test_mc_tensor, y_test_mc_tensor)\n",
    "train_loader_mc = DataLoader(train_dataset_mc, batch_size=batch_size, shuffle=True)\n",
    "test_loader_mc = DataLoader(test_dataset_mc, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3470de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Model (Binary)\n",
    "\n",
    "class CNN_BiLSTM_Binary(nn.Module):\n",
    "    def __init__(self, input_size=1, seq_len=32, hidden_dim=64, lstm_layers=1):\n",
    "        super(CNN_BiLSTM_Binary, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_size, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lstm = nn.LSTM(input_size=16, hidden_size=hidden_dim, num_layers=lstm_layers, \n",
    "                            batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, channels] -> transpose for Conv1d\n",
    "        x = x.transpose(1, 2)  # [batch_size, channels, seq_len]\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = x.transpose(1, 2)  # back to [batch_size, seq_len, channels]\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]  # last time step output\n",
    "        out = self.fc(out)\n",
    "        return out.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc1ab97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm.notebook import tqdm\n",
    "import copy\n",
    "\n",
    "# Validation Split from test\n",
    "X_val_seq, X_test_final_seq, y_val_tensor, y_test_final_tensor = train_test_split(\n",
    "    X_test_seq, y_test_tensor, test_size=0.8, random_state=42, stratify=y_test_tensor\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(TensorDataset(X_val_seq, y_val_tensor), batch_size=batch_size)\n",
    "test_loader_final = DataLoader(TensorDataset(X_test_final_seq, y_test_final_tensor), batch_size=batch_size)\n",
    "\n",
    "# Model, Optimizer with Weight Decay, Loss\n",
    "model_bin = CNN_BiLSTM_Binary().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model_bin.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "# Training parameters\n",
    "epochs = 100\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "no_improve_epochs = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model_bin.train()\n",
    "    total_train_loss = 0\n",
    "    train_loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\", leave=False)\n",
    "\n",
    "    for xb, yb in train_loop:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        preds = model_bin(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model_bin.parameters(), max_norm=2.0)  # ⬅ Gradient Clipping\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    train_loss_epoch = total_train_loss / len(train_loader)\n",
    "    train_losses.append(train_loss_epoch)\n",
    "\n",
    "    # Validation\n",
    "    model_bin.eval()\n",
    "    total_val_loss = 0\n",
    "    val_loop = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loop:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds = model_bin(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    val_loss_epoch = total_val_loss / len(val_loader)\n",
    "    val_losses.append(val_loss_epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss_epoch:.6f} | Val Loss: {val_loss_epoch:.6f}\")\n",
    "\n",
    "    # Early stopping logic\n",
    "    if val_loss_epoch < best_val_loss:\n",
    "        best_val_loss = val_loss_epoch\n",
    "        best_model_state = copy.deepcopy(model_bin.state_dict())\n",
    "        no_improve_epochs = 0\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        if no_improve_epochs >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1} (no improvement in {patience} epochs).\")\n",
    "            break\n",
    "\n",
    "# # Load best model\n",
    "# if best_model_state is not None:\n",
    "#     model_bin.load_state_dict(best_model_state)\n",
    "\n",
    "# Plot training vs validation loss\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final Evaluation\n",
    "model_bin.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader_final:\n",
    "        xb = xb.to(device)\n",
    "        preds = model_bin(xb).cpu().numpy()\n",
    "        preds = (preds > 0.5).astype(int)\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(yb.numpy())\n",
    "\n",
    "print(\"\\nClassification Report on Final Test Set:\")\n",
    "print(classification_report(all_labels, all_preds, digits=4))\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix - Binary Classification\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b316e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Class distribution in training set\n",
    "unique_classes, counts = np.unique(y_train_mc_tensor.numpy(), return_counts=True)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(unique_classes, counts)\n",
    "plt.title(\"Class Distribution in y_train_mc\")\n",
    "plt.xlabel(\"Class Index\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Class Counts:\\n\", dict(zip(unique_classes, counts)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f690843",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Label dtype: {y_train_mc_tensor.dtype}\")\n",
    "print(f\"Min label: {y_train_mc_tensor.min().item()}\")\n",
    "print(f\"Max label: {y_train_mc_tensor.max().item()}\")\n",
    "print(f\"Number of Classes: {len(np.unique(y_train_mc_tensor.numpy()))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ebe62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_BiLSTM_Multiclass(nn.Module):\n",
    "    def __init__(self, input_size=1, seq_len=32, hidden_dim=64, lstm_layers=1, num_classes=10):\n",
    "        super(CNN_BiLSTM_Multiclass, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_size, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lstm = nn.LSTM(input_size=16, hidden_size=hidden_dim, num_layers=lstm_layers,\n",
    "                            batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, num_classes)  # no activation (CrossEntropyLoss expects raw logits)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)  # [batch_size, channels, seq_len]\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = x.transpose(1, 2)  # [batch_size, seq_len, channels]\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]  # take last time step\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5210c4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn.functional as F\n",
    "\n",
    "# class FocalLoss(nn.Module):\n",
    "#     def __init__(self, gamma=2.0, weight=None):\n",
    "#         super(FocalLoss, self).__init__()\n",
    "#         self.gamma = gamma\n",
    "#         self.weight = weight  # Tensor of shape [num_classes]\n",
    "\n",
    "#     def forward(self, input, target):\n",
    "#         ce_loss = F.cross_entropy(input, target, weight=self.weight, reduction='none')\n",
    "#         pt = torch.exp(-ce_loss)  # pt = softmax prob of correct class\n",
    "#         focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "#         return focal_loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170acab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Get class frequencies\n",
    "class_counts = np.bincount(y_train_mc_tensor.numpy())\n",
    "class_weights = 1.0 / class_counts\n",
    "class_weights = class_weights / class_weights.sum()  # Normalize\n",
    "\n",
    "# Convert to tensor for CrossEntropyLoss\n",
    "weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d8188b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mc = CNN_BiLSTM_Multiclass(num_classes=len(class_weights)).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "# criterion = FocalLoss(gamma=2.0, weight=weights_tensor)\n",
    "\n",
    "optimizer = torch.optim.Adam(model_mc.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc105a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# (Optional) Validation split from test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_val_mc, X_test_final_mc, y_val_mc, y_test_final_mc = train_test_split(\n",
    "    X_test_mc_tensor, y_test_mc_tensor, test_size=0.8, stratify=y_test_mc_tensor, random_state=42\n",
    ")\n",
    "\n",
    "val_loader_mc = DataLoader(TensorDataset(X_val_mc, y_val_mc), batch_size=batch_size)\n",
    "test_loader_mc_final = DataLoader(TensorDataset(X_test_final_mc, y_test_final_mc), batch_size=batch_size)\n",
    "\n",
    "# # Training\n",
    "# epochs = 100\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     model_mc.train()\n",
    "#     train_loss = 0.0\n",
    "#     train_loop = tqdm(train_loader_mc, desc=f\"Epoch {epoch+1}/{epochs} [Train]\", leave=False)\n",
    "\n",
    "#     for xb, yb in train_loop:\n",
    "#         xb, yb = xb.to(device), yb.to(device).long()\n",
    "#         optimizer.zero_grad()\n",
    "#         preds = model_mc(xb)\n",
    "#         loss = criterion(preds, yb)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         train_loss += loss.item()\n",
    "\n",
    "#     train_losses.append(train_loss / len(train_loader_mc))\n",
    "    \n",
    "#     for param_group in optimizer.param_groups:  #(Scheduler er LR dekhar jonno per epoch)\n",
    "#         print(f\"Current Learning Rate: {param_group['lr']:.6e}\")\n",
    "\n",
    "\n",
    "#     # Validation\n",
    "#     model_mc.eval()\n",
    "#     val_loss = 0.0\n",
    "#     val_loop = tqdm(val_loader_mc, desc=f\"Epoch {epoch+1}/{epochs} [Val]\", leave=False)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for xb, yb in val_loop:\n",
    "#             xb, yb = xb.to(device), yb.to(device).long()\n",
    "#             preds = model_mc(xb)\n",
    "#             loss = criterion(preds, yb)\n",
    "#             val_loss += loss.item()\n",
    "\n",
    "#     val_losses.append(val_loss / len(val_loader_mc))\n",
    "\n",
    "#     # Step the scheduler\n",
    "#     scheduler.step(val_losses[-1])  # Pass the latest val_loss\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_losses[-1]:.6f}, Val Loss: {val_losses[-1]:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6585fa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping Setup\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "no_improve_count = 0\n",
    "best_model_state = None  # Will store the best model weights\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model_mc.train()\n",
    "    train_loss = 0.0\n",
    "    train_loop = tqdm(train_loader_mc, desc=f\"Epoch {epoch+1}/{epochs} [Train]\", leave=False)\n",
    "\n",
    "    for xb, yb in train_loop:\n",
    "        xb, yb = xb.to(device), yb.to(device).long()\n",
    "        optimizer.zero_grad()\n",
    "        preds = model_mc(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_losses.append(train_loss / len(train_loader_mc))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(f\"Current Learning Rate: {param_group['lr']:.6e}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model_mc.eval()\n",
    "    val_loss = 0.0\n",
    "    val_loop = tqdm(val_loader_mc, desc=f\"Epoch {epoch+1}/{epochs} [Val]\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loop:\n",
    "            xb, yb = xb.to(device), yb.to(device).long()\n",
    "            preds = model_mc(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss_epoch = val_loss / len(val_loader_mc)\n",
    "    val_losses.append(val_loss_epoch)\n",
    "\n",
    "    # Scheduler step\n",
    "    scheduler.step(val_loss_epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_losses[-1]:.6f}, Val Loss: {val_loss_epoch:.6f}\")\n",
    "\n",
    "    # Early stopping logic\n",
    "    if val_loss_epoch < best_val_loss:\n",
    "        best_val_loss = val_loss_epoch\n",
    "        best_model_state = model_mc.state_dict()\n",
    "        no_improve_count = 0\n",
    "    else:\n",
    "        no_improve_count += 1\n",
    "        if no_improve_count >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1} (no improvement in {patience} epochs).\")\n",
    "            break\n",
    "\n",
    "# Restore best model\n",
    "if best_model_state is not None:\n",
    "    model_mc.load_state_dict(best_model_state)\n",
    "    print(\"Loaded best model weights from early stopping checkpoint.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f353a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1630a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "model_mc.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader_mc_final:\n",
    "        xb = xb.to(device)\n",
    "        preds = model_mc(xb)\n",
    "        preds = torch.argmax(preds, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(yb.numpy())\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report on Final Test Set:\")\n",
    "print(classification_report(all_labels, all_preds, digits=4))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix - Multi-class Classification\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdb6e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(\"Predicted class distribution:\", np.unique(all_preds, return_counts=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MAFL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
