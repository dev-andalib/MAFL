{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365a2182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data_utils\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_auc_score, \n",
    "    roc_curve\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a970fc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_parquet(r\"E:\\Thesis\\Defence\\Datasets\\CIC_IDS_2017_Binary_label_is_Label.parquet\")\n",
    "print(f\"Dataset CIC IDS 2017 Shape: {df2.shape}\")\n",
    "display(df2.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84051be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x='Label', data=df2)\n",
    "plt.title('Label Distribution')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4e564d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bin2 = df2.drop(columns=['Label'])\n",
    "y_bin2 = df2['Label'] \n",
    "\n",
    "X_train_bin, X_test_bin, y_train_bin, y_test_bin = train_test_split(\n",
    "    x_bin2, y_bin2, test_size=0.2, random_state=42, stratify=y_bin2\n",
    ")\n",
    "print(\"Class distribution before:\", Counter(y_train_bin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286c327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data_utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "class ImprovedAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dims=[64, 32], dropout_rate=0.1):\n",
    "        super(ImprovedAutoencoder, self).__init__()\n",
    "        \n",
    "        \n",
    "        encoder_layers = []\n",
    "        decoder_layers = []\n",
    "        \n",
    "\n",
    "        prev_dim = input_dim\n",
    "        for dim in encoding_dims:\n",
    "            encoder_layers.extend([\n",
    "                nn.Linear(prev_dim, dim),\n",
    "                nn.BatchNorm1d(dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_dim = dim\n",
    "    \n",
    "        encoder_layers = encoder_layers[:-1]\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        prev_dim = encoding_dims[-1]\n",
    "        for dim in reversed(encoding_dims[:-1]):\n",
    "            decoder_layers.extend([\n",
    "                nn.Linear(prev_dim, dim),\n",
    "                nn.BatchNorm1d(dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_dim = dim\n",
    "        \n",
    "        decoder_layers.extend([\n",
    "            nn.Linear(prev_dim, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        ])\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "class AutoencoderEvaluator:\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "    \n",
    "    def comprehensive_evaluation(self, X_train, X_val, X_test, feature_names=None):\n",
    "        \"\"\"Comprehensive evaluation avoiding data leakage\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # 1. Reconstruction Quality\n",
    "        results['reconstruction'] = self._evaluate_reconstruction(X_train, X_val, X_test)\n",
    "        \n",
    "        # 2. Feature Preservation (proper evaluation)\n",
    "        results['feature_preservation'] = self._evaluate_feature_preservation(\n",
    "            X_train, X_val, X_test, feature_names\n",
    "        )\n",
    "        \n",
    "        # 3. Latent Space Analysis\n",
    "        results['latent_analysis'] = self._analyze_latent_space(X_train, X_val, X_test)\n",
    "        \n",
    "        # 4. Comparison with PCA\n",
    "        results['pca_comparison'] = self._compare_with_pca(X_train, X_val, X_test)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _evaluate_reconstruction(self, X_train, X_val, X_test):\n",
    "        \"\"\"Evaluate reconstruction quality on all splits\"\"\"\n",
    "        self.model.eval()\n",
    "        reconstruction_metrics = {}\n",
    "        \n",
    "        for name, X in [('train', X_train), ('val', X_val), ('test', X_test)]:\n",
    "            with torch.no_grad():\n",
    "                X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "                reconstructed = self.model(X_tensor).cpu().numpy()\n",
    "                \n",
    "                mse = mean_squared_error(X, reconstructed)\n",
    "                reconstruction_metrics[f'{name}_mse'] = mse\n",
    "                reconstruction_metrics[f'{name}_rmse'] = np.sqrt(mse)\n",
    "        \n",
    "        return reconstruction_metrics\n",
    "    \n",
    "    def _evaluate_feature_preservation(self, X_train, X_val, X_test, feature_names):\n",
    "        \"\"\"Proper feature preservation evaluation using separate test set\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Get latent representations\n",
    "            X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(self.device)\n",
    "            X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(self.device)\n",
    "            \n",
    "            latent_train = self.model.encode(X_train_tensor).cpu().numpy()\n",
    "            latent_test = self.model.encode(X_test_tensor).cpu().numpy()\n",
    "        \n",
    "        # Evaluate each feature's preservation\n",
    "        preservation_scores = []\n",
    "        \n",
    "        for i in range(X_train.shape[1]):\n",
    "            # Train linear model on training data\n",
    "            reg = LinearRegression()\n",
    "            reg.fit(latent_train, X_train[:, i])\n",
    "            \n",
    "            # Evaluate on test data\n",
    "            y_pred = reg.predict(latent_test)\n",
    "            r2 = r2_score(X_test[:, i], y_pred)\n",
    "            preservation_scores.append(max(0, r2))  # Ensure non-negative\n",
    "        \n",
    "        return {\n",
    "            'individual_scores': preservation_scores,\n",
    "            'mean_preservation': np.mean(preservation_scores),\n",
    "            'median_preservation': np.median(preservation_scores),\n",
    "            'features_above_90': sum(np.array(preservation_scores) > 0.9),\n",
    "            'features_above_70': sum(np.array(preservation_scores) > 0.7),\n",
    "            'worst_features': np.argsort(preservation_scores)[:5],\n",
    "            'best_features': np.argsort(preservation_scores)[-5:]\n",
    "        }\n",
    "    \n",
    "    def _analyze_latent_space(self, X_train, X_val, X_test):\n",
    "        \"\"\"Analyze properties of the latent space\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X_train, dtype=torch.float32).to(self.device)\n",
    "            latent = self.model.encode(X_tensor).cpu().numpy()\n",
    "        \n",
    "        return {\n",
    "            'latent_dim': latent.shape[1],\n",
    "            'mean_activation': np.mean(latent, axis=0),\n",
    "            'std_activation': np.std(latent, axis=0),\n",
    "            'correlation_matrix': np.corrcoef(latent.T),\n",
    "            'effective_rank': np.linalg.matrix_rank(latent),\n",
    "            'condition_number': np.linalg.cond(latent)\n",
    "        }\n",
    "    \n",
    "    def _compare_with_pca(self, X_train, X_val, X_test):\n",
    "        \"\"\"Compare autoencoder performance with PCA baseline\"\"\"\n",
    "        # Get autoencoder latent dimension\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X_train[:100], dtype=torch.float32).to(self.device)\n",
    "            latent_dim = self.model.encode(X_tensor).shape[1]\n",
    "        \n",
    "        # PCA comparison\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        pca = PCA(n_components=latent_dim)\n",
    "        pca.fit(X_train_scaled)\n",
    "        \n",
    "        # PCA reconstruction\n",
    "        X_test_pca = pca.transform(X_test_scaled)\n",
    "        X_test_reconstructed_pca = pca.inverse_transform(X_test_pca)\n",
    "        X_test_reconstructed_pca = scaler.inverse_transform(X_test_reconstructed_pca)\n",
    "        \n",
    "        # Autoencoder reconstruction\n",
    "        with torch.no_grad():\n",
    "            X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(self.device)\n",
    "            X_test_reconstructed_ae = self.model(X_test_tensor).cpu().numpy()\n",
    "        \n",
    "        return {\n",
    "            'pca_mse': mean_squared_error(X_test, X_test_reconstructed_pca),\n",
    "            'autoencoder_mse': mean_squared_error(X_test, X_test_reconstructed_ae),\n",
    "            'pca_explained_variance_ratio': pca.explained_variance_ratio_,\n",
    "            'pca_cumulative_variance': np.cumsum(pca.explained_variance_ratio_)\n",
    "        }\n",
    "    \n",
    "    def plot_evaluation_results(self, results):\n",
    "        \"\"\"Create comprehensive visualization of results\"\"\"\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        \n",
    "        # 1. Reconstruction Error Comparison\n",
    "        recon = results['reconstruction']\n",
    "        splits = ['train', 'val', 'test']\n",
    "        mse_values = [recon[f'{split}_mse'] for split in splits]\n",
    "        \n",
    "        axes[0, 0].bar(splits, mse_values)\n",
    "        axes[0, 0].set_title('Reconstruction MSE by Split')\n",
    "        axes[0, 0].set_ylabel('MSE')\n",
    "        \n",
    "        # 2. Feature Preservation Distribution\n",
    "        preservation = results['feature_preservation']['individual_scores']\n",
    "        axes[0, 1].hist(preservation, bins=20, alpha=0.7)\n",
    "        axes[0, 1].axvline(np.mean(preservation), color='red', linestyle='--', \n",
    "                          label=f'Mean: {np.mean(preservation):.3f}')\n",
    "        axes[0, 1].set_title('Feature Preservation Score Distribution')\n",
    "        axes[0, 1].set_xlabel('RÂ² Score')\n",
    "        axes[0, 1].legend()\n",
    "        \n",
    "        # 3. PCA vs Autoencoder Comparison\n",
    "        pca_mse = results['pca_comparison']['pca_mse']\n",
    "        ae_mse = results['pca_comparison']['autoencoder_mse']\n",
    "        axes[0, 2].bar(['PCA', 'Autoencoder'], [pca_mse, ae_mse])\n",
    "        axes[0, 2].set_title('PCA vs Autoencoder Reconstruction')\n",
    "        axes[0, 2].set_ylabel('MSE')\n",
    "        \n",
    "        # 4. PCA Explained Variance\n",
    "        explained_var = results['pca_comparison']['pca_cumulative_variance']\n",
    "        axes[1, 0].plot(range(1, len(explained_var) + 1), explained_var, 'o-')\n",
    "        axes[1, 0].set_title('PCA Cumulative Explained Variance')\n",
    "        axes[1, 0].set_xlabel('Number of Components')\n",
    "        axes[1, 0].set_ylabel('Cumulative Explained Variance')\n",
    "        axes[1, 0].grid(True)\n",
    "        \n",
    "        # 5. Latent Space Correlation\n",
    "        corr_matrix = results['latent_analysis']['correlation_matrix']\n",
    "        im = axes[1, 1].imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "        axes[1, 1].set_title('Latent Space Correlation Matrix')\n",
    "        plt.colorbar(im, ax=axes[1, 1])\n",
    "        \n",
    "        # 6. Feature Preservation by Index\n",
    "        axes[1, 2].plot(preservation)\n",
    "        axes[1, 2].axhline(y=0.9, color='red', linestyle='--', alpha=0.7, label='90% threshold')\n",
    "        axes[1, 2].axhline(y=0.7, color='orange', linestyle='--', alpha=0.7, label='70% threshold')\n",
    "        axes[1, 2].set_title('Feature Preservation by Feature Index')\n",
    "        axes[1, 2].set_xlabel('Feature Index')\n",
    "        axes[1, 2].set_ylabel('RÂ² Score')\n",
    "        axes[1, 2].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "def train_and_evaluate_autoencoder(X_train, X_val, X_test, device):\n",
    "    \"\"\"Complete training and evaluation pipeline\"\"\"\n",
    "    \n",
    "    # Initialize improved model\n",
    "    input_dim = X_train.shape[1]\n",
    "    model = ImprovedAutoencoder(input_dim, encoding_dims=[64, 32], dropout_rate=0.1).to(device)\n",
    "    \n",
    "    # Training setup\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_loader = data_utils.DataLoader(\n",
    "        data_utils.TensorDataset(train_tensor, train_tensor),\n",
    "        batch_size=512, shuffle=True\n",
    "    )\n",
    "    val_loader = data_utils.DataLoader(\n",
    "        data_utils.TensorDataset(val_tensor, val_tensor),\n",
    "        batch_size=512, shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Training loop with better practices\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_losses, val_losses = [], []\n",
    "    \n",
    "    for epoch in range(100):  # Increased epochs\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_x, _ in train_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            reconstructed = model(batch_x)\n",
    "            loss = criterion(reconstructed, batch_x)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, _ in val_loader:\n",
    "                batch_x = batch_x.to(device)\n",
    "                reconstructed = model(batch_x)\n",
    "                loss = criterion(reconstructed, batch_x)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        # Record losses\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            torch.save(model.state_dict(), r'E:\\Thesis\\Defence\\ModelWeights\\AE_CIC2017.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= 10:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}: Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}\")\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(r'E:\\Thesis\\Defence\\ModelWeights\\AE_CIC2017.pth'))\n",
    "    \n",
    "    # Comprehensive evaluation\n",
    "    evaluator = AutoencoderEvaluator(model, device)\n",
    "    results = evaluator.comprehensive_evaluation(X_train, X_val, X_test)\n",
    "    \n",
    "    # Plot results\n",
    "    evaluator.plot_evaluation_results(results)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"COMPREHENSIVE AUTOENCODER EVALUATION\")\n",
    "    \n",
    "    print(f\"\\nReconstruction Quality:\")\n",
    "    for key, value in results['reconstruction'].items():\n",
    "        print(f\"  {key}: {value:.6f}\")\n",
    "    \n",
    "    print(f\"\\nFeature Preservation:\")\n",
    "    fp = results['feature_preservation']\n",
    "    print(f\"  Mean preservation: {fp['mean_preservation']:.4f}\")\n",
    "    print(f\"  Median preservation: {fp['median_preservation']:.4f}\")\n",
    "    print(f\"  Features >90% preserved: {fp['features_above_90']}\")\n",
    "    print(f\"  Features >70% preserved: {fp['features_above_70']}\")\n",
    "    \n",
    "    print(f\"\\nComparison with PCA:\")\n",
    "    pc = results['pca_comparison']\n",
    "    print(f\"  PCA MSE: {pc['pca_mse']:.6f}\")\n",
    "    print(f\"  Autoencoder MSE: {pc['autoencoder_mse']:.6f}\")\n",
    "    print(f\"  Improvement over PCA: {((pc['pca_mse'] - pc['autoencoder_mse']) / pc['pca_mse'] * 100):.2f}%\")\n",
    "    \n",
    "    return model, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bae05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import torch\n",
    "\n",
    "# Your existing data preparation\n",
    "x_bin2 = df2.drop(columns=['Label'])\n",
    "y_bin2 = df2['Label'] \n",
    "\n",
    "# First split: separate out final test set (20%)\n",
    "X_temp, X_test_final, y_temp, y_test_final = train_test_split(\n",
    "    x_bin2, y_bin2, test_size=0.2, random_state=42, stratify=y_bin2\n",
    ")\n",
    "\n",
    "# Second split: divide remaining 80% into train (64%) and validation (16%)\n",
    "# This gives us 64% train, 16% validation, 20% test\n",
    "X_train_bin, X_val_bin, y_train_bin, y_val_bin = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.2, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"Dataset splits:\")\n",
    "print(f\"Training set: {X_train_bin.shape[0]} samples ({X_train_bin.shape[0]/len(df2)*100:.1f}%)\")\n",
    "print(f\"Validation set: {X_val_bin.shape[0]} samples ({X_val_bin.shape[0]/len(df2)*100:.1f}%)\")\n",
    "print(f\"Test set: {X_test_final.shape[0]} samples ({X_test_final.shape[0]/len(df2)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nClass distribution:\")\n",
    "print(\"Training:\", Counter(y_train_bin))\n",
    "print(\"Validation:\", Counter(y_val_bin))\n",
    "print(\"Test:\", Counter(y_test_final))\n",
    "\n",
    "# Convert to numpy arrays for the autoencoder\n",
    "X_train = X_train_bin.values\n",
    "X_val = X_val_bin.values  \n",
    "X_test = X_test_final.values\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "# Now you can use the improved autoencoder\n",
    "print(\"\\nTraining improved autoencoder...\")\n",
    "model, results = train_and_evaluate_autoencoder(X_train, X_val, X_test, device)\n",
    "\n",
    "# After training, you can extract compressed features for your classification task\n",
    "print(\"\\nExtracting compressed features for classification...\")\n",
    "\n",
    "# Get compressed representations\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_train_compressed = model.encode(torch.tensor(X_train, dtype=torch.float32).to(device)).cpu().numpy()\n",
    "    X_val_compressed = model.encode(torch.tensor(X_val, dtype=torch.float32).to(device)).cpu().numpy()\n",
    "    X_test_compressed = model.encode(torch.tensor(X_test, dtype=torch.float32).to(device)).cpu().numpy()\n",
    "\n",
    "print(f\"Original feature dimensions: {X_train.shape[1]}\")\n",
    "print(f\"Compressed feature dimensions: {X_train_compressed.shape[1]}\")\n",
    "print(f\"Compression ratio: {X_train_compressed.shape[1]/X_train.shape[1]:.3f}\")\n",
    "\n",
    "# Optional: Save the compressed features for later use\n",
    "# np.save('X_train_compressed.npy', X_train_compressed)\n",
    "# np.save('X_val_compressed.npy', X_val_compressed) \n",
    "# np.save('X_test_compressed.npy', X_test_compressed)\n",
    "# np.save('y_train.npy', y_train_bin.values)\n",
    "# np.save('y_val.npy', y_val_bin.values)\n",
    "# np.save('y_test.npy', y_test_final.values)\n",
    "\n",
    "# print(\"Compressed features saved to disk.\")\n",
    "\n",
    "# Example: Quick classification test on compressed features\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print(\"\\nTesting classification performance on compressed features...\")\n",
    "\n",
    "# Train classifier on compressed features\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train_compressed, y_train_bin)\n",
    "\n",
    "# Evaluate\n",
    "val_pred = clf.predict(X_val_compressed)\n",
    "test_pred = clf.predict(X_test_compressed)\n",
    "\n",
    "print(f\"\\nClassification Results on Compressed Features:\")\n",
    "print(f\"Validation Accuracy: {accuracy_score(y_val_bin, val_pred):.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test_final, test_pred):.4f}\")\n",
    "\n",
    "# Compare with original features\n",
    "print(\"\\nComparing with original features...\")\n",
    "clf_original = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf_original.fit(X_train, y_train_bin)\n",
    "\n",
    "val_pred_orig = clf_original.predict(X_val)\n",
    "test_pred_orig = clf_original.predict(X_test)\n",
    "\n",
    "print(f\"Original Features - Validation Accuracy: {accuracy_score(y_val_bin, val_pred_orig):.4f}\")\n",
    "print(f\"Original Features - Test Accuracy: {accuracy_score(y_test_final, test_pred_orig):.4f}\")\n",
    "\n",
    "print(f\"\\nAccuracy difference (compressed vs original):\")\n",
    "print(f\"Validation: {accuracy_score(y_val_bin, val_pred) - accuracy_score(y_val_bin, val_pred_orig):.4f}\")\n",
    "print(f\"Test: {accuracy_score(y_test_final, test_pred) - accuracy_score(y_test_final, test_pred_orig):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8a7233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x_bin2 = df2.drop(columns=['Label'])\n",
    "y_bin2 = df2['Label'] \n",
    "\n",
    "# First split: separate out final test set (20%)\n",
    "X_temp, X_test_final, y_temp, y_test_final = train_test_split(\n",
    "    x_bin2, y_bin2, test_size=0.2, random_state=42, stratify=y_bin2\n",
    ")\n",
    "\n",
    "# Second split: divide remaining 80% into train (64%) and validation (16%)\n",
    "# This gives us 64% train, 16% validation, 20% test\n",
    "X_train_bin, X_val_bin, y_train_bin, y_val_bin = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.2, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "def analyze_dataset_complexity(X_data, feature_names=None):\n",
    "    \"\"\"Comprehensive analysis of why your autoencoder works 'too well'\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"DATASET COMPLEXITY ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Sparsity Analysis\n",
    "    sparsity_ratio = np.mean(X_data == 0)\n",
    "    near_zero_ratio = np.mean(np.abs(X_data) < 1e-6)\n",
    "    \n",
    "    print(f\"\\n1. SPARSITY ANALYSIS:\")\n",
    "    print(f\"   Exact zeros: {sparsity_ratio:.1%}\")\n",
    "    print(f\"   Near-zero values (< 1e-6): {near_zero_ratio:.1%}\")\n",
    "    print(f\"   Effective non-zero features: {100-near_zero_ratio*100:.1f}%\")\n",
    "    \n",
    "    # 2. Value Range Analysis\n",
    "    print(f\"\\n2. VALUE RANGE ANALYSIS:\")\n",
    "    print(f\"   Min value: {np.min(X_data):.2e}\")\n",
    "    print(f\"   Max value: {np.max(X_data):.2e}\")\n",
    "    print(f\"   Mean value: {np.mean(X_data):.2e}\")\n",
    "    print(f\"   Std deviation: {np.std(X_data):.2e}\")\n",
    "    \n",
    "    # Count features by magnitude\n",
    "    tiny_features = np.sum(np.max(np.abs(X_data), axis=0) < 1e-5)\n",
    "    small_features = np.sum((np.max(np.abs(X_data), axis=0) >= 1e-5) & \n",
    "                           (np.max(np.abs(X_data), axis=0) < 1e-2))\n",
    "    normal_features = np.sum(np.max(np.abs(X_data), axis=0) >= 1e-2)\n",
    "    \n",
    "    print(f\"   Features with max < 1e-5: {tiny_features}\")\n",
    "    print(f\"   Features with max 1e-5 to 1e-2: {small_features}\")\n",
    "    print(f\"   Features with max >= 1e-2: {normal_features}\")\n",
    "    \n",
    "    # 3. Correlation Analysis\n",
    "    corr_matrix = np.corrcoef(X_data.T)\n",
    "    high_corr_pairs = np.sum(np.abs(corr_matrix) > 0.9) - X_data.shape[1]  # Subtract diagonal\n",
    "    moderate_corr_pairs = np.sum((np.abs(corr_matrix) > 0.7) & (np.abs(corr_matrix) <= 0.9))\n",
    "    \n",
    "    print(f\"\\n3. CORRELATION ANALYSIS:\")\n",
    "    print(f\"   Highly correlated pairs (>0.9): {high_corr_pairs//2}\")\n",
    "    print(f\"   Moderately correlated pairs (0.7-0.9): {moderate_corr_pairs//2}\")\n",
    "    print(f\"   Max correlation: {np.max(corr_matrix[corr_matrix < 1]):.3f}\")\n",
    "    \n",
    "    # 4. Effective Dimensionality\n",
    "    # Use PCA to understand true dimensionality\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_data)\n",
    "    pca = PCA()\n",
    "    pca.fit(X_scaled)\n",
    "    \n",
    "    # Find components that explain 95% and 99% variance\n",
    "    cumvar = np.cumsum(pca.explained_variance_ratio_)\n",
    "    dim_95 = np.argmax(cumvar >= 0.95) + 1\n",
    "    dim_99 = np.argmax(cumvar >= 0.99) + 1\n",
    "    \n",
    "    print(f\"\\n4. EFFECTIVE DIMENSIONALITY:\")\n",
    "    print(f\"   Original dimensions: {X_data.shape[1]}\")\n",
    "    print(f\"   Dimensions for 95% variance: {dim_95}\")\n",
    "    print(f\"   Dimensions for 99% variance: {dim_99}\")\n",
    "    print(f\"   First component explains: {pca.explained_variance_ratio_[0]:.1%}\")\n",
    "    print(f\"   First 5 components explain: {np.sum(pca.explained_variance_ratio_[:5]):.1%}\")\n",
    "    \n",
    "    # 5. Feature Variance Analysis\n",
    "    feature_vars = np.var(X_data, axis=0)\n",
    "    zero_var_features = np.sum(feature_vars < 1e-10)\n",
    "    low_var_features = np.sum((feature_vars >= 1e-10) & (feature_vars < 1e-6))\n",
    "    \n",
    "    print(f\"\\n5. FEATURE VARIANCE ANALYSIS:\")\n",
    "    print(f\"   Zero/near-zero variance features: {zero_var_features}\")\n",
    "    print(f\"   Very low variance features: {low_var_features}\")\n",
    "    print(f\"   Informative features: {X_data.shape[1] - zero_var_features - low_var_features}\")\n",
    "    \n",
    "    return {\n",
    "        'sparsity_ratio': sparsity_ratio,\n",
    "        'near_zero_ratio': near_zero_ratio,\n",
    "        'effective_dims_95': dim_95,\n",
    "        'effective_dims_99': dim_99,\n",
    "        'high_corr_pairs': high_corr_pairs//2,\n",
    "        'zero_var_features': zero_var_features,\n",
    "        'pca_first_component': pca.explained_variance_ratio_[0],\n",
    "        'corr_matrix': corr_matrix,\n",
    "        'pca_components': pca.explained_variance_ratio_\n",
    "    }\n",
    "\n",
    "def visualize_dataset_issues(X_data, analysis_results):\n",
    "    \"\"\"Create visualizations showing why autoencoder works 'too well'\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Feature value distribution\n",
    "    axes[0, 0].hist(X_data.flatten(), bins=50, alpha=0.7, log=True)\n",
    "    axes[0, 0].set_title('Distribution of All Feature Values\\n(Log Scale)')\n",
    "    axes[0, 0].set_xlabel('Feature Value')\n",
    "    axes[0, 0].set_ylabel('Frequency (log)')\n",
    "    \n",
    "    # 2. Feature variance distribution\n",
    "    feature_vars = np.var(X_data, axis=0)\n",
    "    axes[0, 1].hist(np.log10(feature_vars + 1e-12), bins=30, alpha=0.7)\n",
    "    axes[0, 1].set_title('Feature Variance Distribution\\n(Log10 Scale)')\n",
    "    axes[0, 1].set_xlabel('Log10(Variance)')\n",
    "    axes[0, 1].set_ylabel('Number of Features')\n",
    "    \n",
    "    # 3. Correlation matrix heatmap\n",
    "    corr_matrix = analysis_results['corr_matrix']\n",
    "    im = axes[0, 2].imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    axes[0, 2].set_title('Feature Correlation Matrix')\n",
    "    plt.colorbar(im, ax=axes[0, 2])\n",
    "    \n",
    "    # 4. PCA explained variance\n",
    "    pca_vars = analysis_results['pca_components']\n",
    "    axes[1, 0].plot(range(1, min(21, len(pca_vars)+1)), pca_vars[:20], 'o-')\n",
    "    axes[1, 0].set_title('PCA Explained Variance\\n(First 20 Components)')\n",
    "    axes[1, 0].set_xlabel('Component')\n",
    "    axes[1, 0].set_ylabel('Explained Variance Ratio')\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    # 5. Cumulative explained variance\n",
    "    cumvar = np.cumsum(pca_vars)\n",
    "    axes[1, 1].plot(range(1, min(51, len(cumvar)+1)), cumvar[:50], 'o-')\n",
    "    axes[1, 1].axhline(y=0.95, color='red', linestyle='--', label='95%')\n",
    "    axes[1, 1].axhline(y=0.99, color='orange', linestyle='--', label='99%')\n",
    "    axes[1, 1].set_title('Cumulative Explained Variance')\n",
    "    axes[1, 1].set_xlabel('Number of Components')\n",
    "    axes[1, 1].set_ylabel('Cumulative Variance')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "    \n",
    "    # 6. Feature magnitude distribution\n",
    "    max_vals = np.max(np.abs(X_data), axis=0)\n",
    "    axes[1, 2].hist(np.log10(max_vals + 1e-12), bins=30, alpha=0.7)\n",
    "    axes[1, 2].set_title('Feature Maximum Value Distribution\\n(Log10 Scale)')\n",
    "    axes[1, 2].set_xlabel('Log10(Max Absolute Value)')\n",
    "    axes[1, 2].set_ylabel('Number of Features')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def test_reconstruction_difficulty(X_data):\n",
    "    \"\"\"Test how easy it is to reconstruct this data\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RECONSTRUCTION DIFFICULTY TEST\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Test 1: Random reconstruction\n",
    "    X_random = np.random.normal(0, np.std(X_data), X_data.shape)\n",
    "    random_mse = np.mean((X_data - X_random)**2)\n",
    "    \n",
    "    # Test 2: Mean reconstruction\n",
    "    X_mean = np.full_like(X_data, np.mean(X_data, axis=0))\n",
    "    mean_mse = np.mean((X_data - X_mean)**2)\n",
    "    \n",
    "    # Test 3: Median reconstruction\n",
    "    X_median = np.full_like(X_data, np.median(X_data, axis=0))\n",
    "    median_mse = np.mean((X_data - X_median)**2)\n",
    "    \n",
    "    # Test 4: Zero reconstruction (predicting all zeros)\n",
    "    zero_mse = np.mean(X_data**2)\n",
    "    \n",
    "    print(f\"\\nBaseline Reconstruction MSE:\")\n",
    "    print(f\"   Random noise: {random_mse:.2e}\")\n",
    "    print(f\"   Feature means: {mean_mse:.2e}\")\n",
    "    print(f\"   Feature medians: {median_mse:.2e}\")\n",
    "    print(f\"   All zeros: {zero_mse:.2e}\")\n",
    "    \n",
    "    # If your autoencoder gets much lower MSE than these baselines,\n",
    "    # it might actually be learning something useful\n",
    "    print(f\"\\nInterpretation:\")\n",
    "    if zero_mse < 1e-5:\n",
    "        print(\"   âš ï¸  Data is extremely sparse - even predicting zeros gives very low MSE\")\n",
    "    if mean_mse < 1e-5:\n",
    "        print(\"   âš ï¸  Predicting feature means gives very low MSE - data has low variance\")\n",
    "    \n",
    "    return {\n",
    "        'random_mse': random_mse,\n",
    "        'mean_mse': mean_mse,\n",
    "        'median_mse': median_mse,\n",
    "        'zero_mse': zero_mse\n",
    "    }\n",
    "\n",
    "def why_simple_autoencoder_works(X_data):\n",
    "    \"\"\"Comprehensive explanation of why your simple autoencoder works\"\"\"\n",
    "    \n",
    "    analysis = analyze_dataset_complexity(X_data)\n",
    "    baseline_mse = test_reconstruction_difficulty(X_data)\n",
    "    visualize_dataset_issues(X_data, analysis)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"WHY YOUR SIMPLE AUTOENCODER 'WORKS'\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    reasons = []\n",
    "    \n",
    "    if analysis['sparsity_ratio'] > 0.5:\n",
    "        reasons.append(\"ðŸ”¸ Extreme sparsity - most values are zero\")\n",
    "    \n",
    "    if analysis['near_zero_ratio'] > 0.8:\n",
    "        reasons.append(\"ðŸ”¸ Most values are essentially zero (< 1e-6)\")\n",
    "    \n",
    "    if analysis['effective_dims_95'] < analysis['effective_dims_99'] * 0.7:\n",
    "        reasons.append(\"ðŸ”¸ Very low effective dimensionality\")\n",
    "    \n",
    "    if analysis['pca_first_component'] > 0.5:\n",
    "        reasons.append(\"ðŸ”¸ First PCA component dominates variance\")\n",
    "    \n",
    "    if analysis['zero_var_features'] > X_data.shape[1] * 0.2:\n",
    "        reasons.append(\"ðŸ”¸ Many features have zero/near-zero variance\")\n",
    "    \n",
    "    if baseline_mse['zero_mse'] < 1e-4:\n",
    "        reasons.append(\"ðŸ”¸ Predicting all zeros gives very low MSE\")\n",
    "    \n",
    "    print(\"\\nYour autoencoder works 'too well' because:\")\n",
    "    for reason in reasons:\n",
    "        print(f\"   {reason}\")\n",
    "    \n",
    "    print(f\"\\nðŸš¨ CRITICAL ISSUES:\")\n",
    "    print(f\"   â€¢ Your data is too easy to reconstruct\")\n",
    "    print(f\"   â€¢ Low MSE doesn't mean good feature learning\")\n",
    "    print(f\"   â€¢ Simple linear projection might work just as well\")\n",
    "    print(f\"   â€¢ Autoencoder might be learning trivial patterns\")\n",
    "    \n",
    "    print(f\"\\nðŸ’¡ RECOMMENDATIONS:\")\n",
    "    print(f\"   1. Proper feature scaling/normalization\")\n",
    "    print(f\"   2. Remove zero-variance features\")\n",
    "    print(f\"   3. Compare with PCA baseline\")\n",
    "    print(f\"   4. Test on downstream classification task\")\n",
    "    print(f\"   5. Use more challenging evaluation metrics\")\n",
    "    \n",
    "    return analysis, baseline_mse\n",
    "\n",
    "# Usage with your data:\n",
    "analysis, baselines = why_simple_autoencoder_works(X_train_bin.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a989f35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data_utils\n",
    "import os\n",
    "import torch.nn.init as init\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Convert data to tensors\n",
    "X_train_tensor = torch.tensor(X_train_bin.values, dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test_bin.values, dtype=torch.float32).to(device)\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 512\n",
    "train_loader = data_utils.DataLoader(\n",
    "    data_utils.TensorDataset(X_train_tensor, X_train_tensor),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = data_utils.DataLoader(\n",
    "    data_utils.TensorDataset(X_test_tensor, X_test_tensor),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Define Autoencoder with Xavier Initialization\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Apply Xavier initialization to the layers\n",
    "        self.apply(self.initialize_weights)\n",
    "\n",
    "    def initialize_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            init.xavier_uniform_(m.weight)  # Apply Xavier initialization\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)  # Initialize biases to zero\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Initialize model\n",
    "input_dim = X_train_bin.shape[1]\n",
    "autoencoder = Autoencoder(input_dim).to(device)\n",
    "\n",
    "# Path to save/load model\n",
    "model_path = \"E:\\Thesis\\Defence\\ModelWeights\\AE_CIC2017.pth\"\n",
    "\n",
    "# Early stopping setup\n",
    "early_stopping_patience = 5\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# If model exists, load it\n",
    "if os.path.exists(model_path):\n",
    "    autoencoder.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    autoencoder.eval()\n",
    "    print(\"Loaded pre-trained Autoencoder from disk.\")\n",
    "else:\n",
    "    # Train the model\n",
    "    optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 50\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    autoencoder.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_train_loss = 0\n",
    "        total_val_loss = 0\n",
    "        \n",
    "        # Training loop\n",
    "        for xb, _ in train_loader:\n",
    "            xb = xb.to(device)  # Ensure data is on the same device as the model\n",
    "            output = autoencoder(xb)\n",
    "            loss = criterion(output, xb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Validation loop\n",
    "        autoencoder.eval()\n",
    "        with torch.no_grad():\n",
    "            for xb, _ in val_loader:\n",
    "                xb = xb.to(device)  # Ensure data is on the same device as the model\n",
    "                output = autoencoder(xb)\n",
    "                loss = criterion(output, xb)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        # Print loss for each epoch\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}\")\n",
    "        \n",
    "        # Early Stopping condition\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            torch.save(autoencoder.state_dict(), model_path)  # Save model\n",
    "            print(\"Model saved (improved validation loss).\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= early_stopping_patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break  # Stop training if no improvement in validation loss\n",
    "\n",
    "        # Switch back to training mode\n",
    "        autoencoder.train()\n",
    "\n",
    "    # Plot Training vs Validation Loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(len(train_losses)), train_losses, label=\"Training Loss\", color='blue')\n",
    "    plt.plot(range(len(val_losses)), val_losses, label=\"Validation Loss\", color='red')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training vs Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Compression analysis\n",
    "    compression_ratio = 32 / input_dim\n",
    "    print(f\"\\nCompression Analysis:\")\n",
    "    print(f\"Original dimensions: {input_dim}\")\n",
    "    print(f\"Compressed dimensions: 32\")\n",
    "    print(f\"Compression ratio: {compression_ratio:.4f}\")\n",
    "    print(f\"Storage reduction: {(1-compression_ratio)*100:.2f}%\")\n",
    "\n",
    "    # Feature importance analysis\n",
    "    with torch.no_grad():\n",
    "        latent_representations = autoencoder.encoder(X_train_tensor).cpu().numpy()\n",
    "\n",
    "    feature_importance = []\n",
    "    for i in range(input_dim):\n",
    "        y = X_train_bin.iloc[:, i].values\n",
    "        model = LinearRegression().fit(latent_representations, y)\n",
    "        r2 = model.score(latent_representations, y)\n",
    "        feature_importance.append(r2)\n",
    "\n",
    "    print(f\"\\nFeature Preservation Analysis:\")\n",
    "    print(f\"Average preservation score: {np.mean(feature_importance):.4f}\")\n",
    "    print(f\"Features with >90% preservation: {sum(np.array(feature_importance) > 0.9)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb032a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed_test = autoencoder(X_test_tensor).cpu().numpy()\n",
    "\n",
    "reconstruction_error = np.mean((X_test_bin.values - reconstructed_test) ** 2, axis=1)\n",
    "\n",
    "print(f\"Mean Reconstruction Error: {reconstruction_error.mean():.6f}\")\n",
    "print(f\"Std of Reconstruction Error: {reconstruction_error.std():.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
